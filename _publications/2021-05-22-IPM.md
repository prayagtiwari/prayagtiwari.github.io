---
title: "Neural variational sparse topic model for sparse explainable text representation"
collection: publications

excerpt: 'we propose a semantic reinforcement neural variational sparse topic model (SR-NSTM) towards explainable and sparse latent text representation learning.' 
date: 2021-05-22
venue: 'Information Processing & Management'
paperurl: 'https://www.sciencedirect.com/science/article/abs/pii/S0306457321001102'
citation: 'Q. Xie, P. Tiwari, D. Gupta, J. Huang, M. Peng, "Neural variational sparse topic model for sparse explainable text representation," in Information Processing & Management, https://doi.org/10.1016/j.ipm.2021.102614.'
---
Texts are the major information carrier for internet users, from which learning the latent representations has important research and practical value. Neural topic models have been proposed and have great performance in extracting interpretable latent topics and representations of texts. However, there remain two major limitations: (1) these methods generally ignore the contextual information of texts and have limited feature representation ability due to the shallow feed-forward network architecture, (2) Sparsity of the representations in topic semantic space is ignored. To address these issues, in this paper, we propose a semantic reinforcement neural variational sparse topic model (SR-NSTM) towards explainable and sparse latent text representation learning. Compared with existing neural topic models, SR-NSTM models the generative process of texts with probabilistic distributions parameterized with neural networks and incorporates Bi-directional LSTM to embed contextual information at the document level. It achieves sparse posterior representations over documents and words with zero-mean Laplace distribution and topics with sparsemax. Moreover, we propose a supervised extension of SR-NSTM via adding the max-margin posterior regularization to tackle the supervised tasks. The neural variational inference method is utilized to learn our models efficiently. Experimental results on Web Snippets, 20Newsgroups, BBC, and Biomedical datasets demonstrate that the contextual information and revisiting generative process can improve the performance, leading to the competitive performance of our models in learning coherent topics and explainable sparse representations for texts.

[Download paper here](https://github.com/prayagtiwari/prayagtiwari.github.io/tree/master/files/IPM.pdf)

Recommended citation:  Q. Xie, P. Tiwari, D. Gupta, J. Huang, M. Peng, "Neural variational sparse topic model for sparse explainable text representation," in Information Processing & Management, https://doi.org/10.1016/j.ipm.2021.102614.
